import re
import os
import time
import aiohttp
import json
import hashlib
import asyncio
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
from astrbot.api.message_components import Plain, Image, Video, File

@register("xhs_parse_hub", "YourName", "å°çº¢ä¹¦å»æ°´å°è§£ææ’ä»¶", "1.3.0")
class XhsParseHub(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url = config.get("api_url", "http://127.0.0.1:5556/xhs/")
        self.enable_cache = config.get("enable_download_cache", True)
        # [æ–°å¢] è¯»å–æ··åˆæ¨¡å¼é…ç½®ï¼Œé»˜è®¤ä¸º True
        self.enable_hybrid = config.get("enable_hybrid_mode", True)
        
        current_plugin_dir = os.path.dirname(os.path.abspath(__file__))
        self.cache_dir = os.path.join(current_plugin_dir, "xhs_cache")
        
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
            
        self.cleanup_task = None

    async def initialize(self):
        logger.info(f"========== å°çº¢ä¹¦æ’ä»¶å¯åŠ¨ (v1.3.0) ==========")
        logger.info(f"API: {self.api_url}")
        logger.info(f"æ¨¡å¼: {'æ··åˆ(ç›¸å†Œ+æ–‡ä»¶)' if self.enable_hybrid else 'çº¯æ–‡ä»¶(åŸå›¾)'}")
        
        if self.enable_cache:
            self.cleanup_task = asyncio.create_task(self._auto_cleanup_loop())

    async def terminate(self):
        if self.cleanup_task:
            self.cleanup_task.cancel()

    async def _auto_cleanup_loop(self):
        while True:
            try:
                await asyncio.sleep(3600)
                if os.path.exists(self.cache_dir):
                    now = time.time()
                    for filename in os.listdir(self.cache_dir):
                        file_path = os.path.join(self.cache_dir, filename)
                        if not os.path.isfile(file_path): continue
                        if now - os.path.getmtime(file_path) > 3600:
                            try: os.remove(file_path)
                            except: pass
            except asyncio.CancelledError: break
            except Exception: await asyncio.sleep(60)

    def extract_url(self, text: str):
        pattern = r'(https?://[^\s]+)'
        match = re.search(pattern, text)
        if match: return match.group(0)
        return None

    def clean_filename(self, title: str) -> str:
        return re.sub(r'[\\/*?:"<>|]', "", title).strip()[:50]

    async def download_file(self, url: str, suffix: str = "") -> str:
        if not url: return None
        try:
            file_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            filename = f"{file_hash}{suffix}"
            file_path = os.path.join(self.cache_dir, filename)

            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                os.utime(file_path, None)
                return file_path

            async with aiohttp.ClientSession() as session:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        content = await resp.read()
                        with open(file_path, 'wb') as f:
                            f.write(content)
                        return file_path
                    else:
                        logger.error(f"ä¸‹è½½å¤±è´¥ {resp.status}: {url}")
                        return None
        except Exception as e:
            logger.error(f"ä¸‹è½½å¼‚å¸¸: {e}")
            return None

    @filter.command("xhs")
    async def xhs_parse(self, event: AstrMessageEvent):
        message_str = event.message_str
        target_url = self.extract_url(message_str)
        
        if not target_url:
            if "http" in message_str: target_url = message_str.strip()
            else:
                yield event.plain_result("âš ï¸ è¯·æä¾›é“¾æ¥ã€‚")
                return

        yield event.plain_result("ğŸ” æ­£åœ¨è§£æ...")

        # --- 1. è¯·æ±‚ API ---
        res_json = None
        try:
            async with aiohttp.ClientSession() as session:
                timeout = aiohttp.ClientTimeout(total=15)
                async with session.post(self.api_url, json={"url": target_url}, timeout=timeout) as resp:
                    if resp.status != 200:
                        yield event.plain_result(f"âŒ è§£æè¯·æ±‚å¤±è´¥: {resp.status}")
                        return
                    res_json = await resp.json()
        except Exception as e:
            yield event.plain_result(f"âŒ è¿æ¥é”™è¯¯: {e}")
            return

        # --- 2. æå–æ•°æ® ---
        data = res_json.get("data")
        if not data:
            msg = res_json.get("message", "æœªçŸ¥é”™è¯¯")
            yield event.plain_result(f"âŒ è§£æå¤±è´¥: {msg}")
            return

        title = data.get("ä½œå“æ ‡é¢˜", "æ— æ ‡é¢˜")
        author = data.get("ä½œè€…æ˜µç§°", "æœªçŸ¥ä½œè€…")
        desc = data.get("ä½œå“æè¿°", "")
        work_type = data.get("ä½œå“ç±»å‹", "")
        download_urls = data.get("ä¸‹è½½åœ°å€", [])
        dynamic_urls = data.get("åŠ¨å›¾åœ°å€", [])
        
        clean_title = self.clean_filename(title)

        # --- 3. æ„å»ºæ–‡æœ¬ ---
        info_text = f"ã€æ ‡é¢˜ã€‘{title}\nã€ä½œè€…ã€‘{author}\n\n{desc}"
        if len(info_text) > 250:
            info_text = info_text[:250] + "...\n(æ–‡æ¡ˆè¿‡é•¿å·²æŠ˜å )"

        video_direct_link = None
        if work_type == "è§†é¢‘" and download_urls:
            video_direct_link = download_urls[0]
            info_text += f"\n\nğŸ”— è§†é¢‘ç›´é“¾:\n{video_direct_link}"

        if work_type == "å›¾æ–‡" and dynamic_urls:
            live_links = [url for url in dynamic_urls if url]
            if live_links:
                info_text += f"\n\nğŸï¸ åŠ¨å›¾ç›´é“¾ ({len(live_links)}ä¸ª):\n"
                for idx, link in enumerate(live_links, 1):
                    info_text += f"{idx}. {link}\n"

        yield event.plain_result(info_text)

        # --- 4. å‘é€åª’ä½“ ---
        if not download_urls:
            yield event.plain_result("âš ï¸ æœªæ‰¾åˆ°èµ„æºã€‚")
            return

        if self.enable_cache:
            # ====== ç¼“å­˜æ¨¡å¼ ======
            if work_type == "è§†é¢‘" and video_direct_link:
                yield event.plain_result("ğŸ“¥ æ­£åœ¨ä¸‹è½½è§†é¢‘...")
                local_path = await self.download_file(video_direct_link, suffix=".mp4")
                
                if local_path:
                    file_size_mb = os.path.getsize(local_path) / (1024 * 1024)
                    if file_size_mb > 49:
                        yield event.plain_result(f"âš ï¸ è§†é¢‘è¿‡å¤§ ({file_size_mb:.1f}MB)ï¼Œè¯·ä½¿ç”¨ç›´é“¾ã€‚")
                    else:
                        yield event.plain_result(f"ğŸ“¤ ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨ä»¥æ–‡ä»¶å‘é€...")
                        try:
                            # è§†é¢‘å¼ºåˆ¶ç”¨ File
                            final_filename = f"{clean_title}.mp4"
                            yield event.chain_result([File(name=final_filename, file=local_path)])
                        except Exception as e:
                            logger.error(f"è§†é¢‘æ–‡ä»¶å‘é€å¤±è´¥: {e}")
                            yield event.plain_result("âš ï¸ å‘é€å¤±è´¥ï¼Œè¯·ä½¿ç”¨ç›´é“¾ã€‚")
                else:
                    yield event.plain_result("âŒ ä¸‹è½½å¤±è´¥ã€‚")

            else: # å›¾æ–‡æ¨¡å¼
                count = len(download_urls)
                yield event.plain_result(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {count} å¼ å›¾ç‰‡...")
                
                # æ‰¹é‡ä¸‹è½½
                local_paths = []
                for i, url in enumerate(download_urls):
                    path = await self.download_file(url, suffix=".jpg")
                    if path: local_paths.append(path)

                if not local_paths:
                    yield event.plain_result("âŒ æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å¤±è´¥ã€‚")
                    return

                # >>>>>>> åˆ†æ”¯ 1: æ··åˆæ¨¡å¼ (Imageç›¸å†Œ + å¤§å›¾File) <<<<<<<
                if self.enable_hybrid:
                    yield event.plain_result("ğŸ“¤ [æ··åˆæ¨¡å¼] æ­£åœ¨å‘é€(ç›¸å†Œ+æ–‡ä»¶)...")
                    album_images = []
                    large_files = []

                    for i, path in enumerate(local_paths):
                        file_size = os.path.getsize(path)
                        final_filename = f"{clean_title}_{i+1}.jpg"

                        if file_size >= 10 * 1024 * 1024:
                            large_files.append(File(name=final_filename, file=path))
                        else:
                            album_images.append(Image.fromFileSystem(path))

                    # 1. å‘é€ç›¸å†Œ (åˆå¹¶)
                    if album_images:
                        batch_size = 10
                        for i in range(0, len(album_images), batch_size):
                            batch = album_images[i:i + batch_size]
                            try:
                                yield event.chain_result(batch)
                                if i + batch_size < len(album_images):
                                    await asyncio.sleep(1)
                            except Exception as e:
                                logger.error(f"ç›¸å†Œå‘é€å¤±è´¥: {e}")
                                yield event.plain_result("âš ï¸ éƒ¨åˆ†ç›¸å†Œå›¾ç‰‡å‘é€å¤±è´¥ã€‚")

                    # 2. å‘é€å¤§æ–‡ä»¶
                    if large_files:
                        yield event.plain_result(f"âš ï¸ æ£€æµ‹åˆ° {len(large_files)} å¼ å¤§å›¾ï¼Œå•ç‹¬å‘é€...")
                        for f in large_files:
                            try:
                                yield event.chain_result([f])
                                await asyncio.sleep(1)
                            except: pass

                # >>>>>>> åˆ†æ”¯ 2: çº¯æ–‡ä»¶æ¨¡å¼ (File Batch) <<<<<<<
                else:
                    yield event.plain_result("ğŸ“¤ [åŸå›¾æ¨¡å¼] æ­£åœ¨å‘é€æ‰€æœ‰æ–‡ä»¶...")
                    file_components = []
                    for i, path in enumerate(local_paths):
                        final_filename = f"{clean_title}_{i+1}.jpg"
                        file_components.append(File(name=final_filename, file=path))
                    
                    # æ‰¹é‡å‘é€æ–‡ä»¶ (è™½ç„¶TGä¼šè§†ä¸ºå•ä¸ªæ–‡ä»¶åˆ—è¡¨ï¼Œä½†ä»£ç é€»è¾‘ä¸Šæˆ‘ä»¬æ‰“åŒ…å‘é€)
                    batch_size = 10
                    for i in range(0, len(file_components), batch_size):
                        batch = file_components[i:i + batch_size]
                        try:
                            yield event.chain_result(batch)
                            if i + batch_size < len(file_components):
                                await asyncio.sleep(2)
                        except Exception as e:
                            logger.error(f"æ–‡ä»¶æ‰¹æ¬¡å‘é€å¤±è´¥: {e}")
                            yield event.plain_result(f"âš ï¸ ç¬¬ {i//batch_size + 1} ç»„æ–‡ä»¶å‘é€å¤±è´¥ã€‚")

        else:
            # ====== æ— ç¼“å­˜æ¨¡å¼ ======
            if work_type == "è§†é¢‘":
                yield event.plain_result("ğŸ¬ æ­£åœ¨å‘é€è§†é¢‘...")
                try:
                    yield event.chain_result([Video.fromURL(video_direct_link)])
                except: yield event.plain_result("âš ï¸ å‘é€å¤±è´¥ã€‚")
            else:
                yield event.plain_result(f"ğŸ–¼ï¸ æ­£åœ¨å‘é€å›¾ç‰‡...")
                for url in download_urls:
                    try:
                        yield event.chain_result([Image.fromURL(url)])
                    except: pass